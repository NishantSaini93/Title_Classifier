{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package wordnet to /home/nishant/nltk_data...\n",
      "[nltk_data]   Package wordnet is already up-to-date!\n",
      "[nltk_data] Downloading package averaged_perceptron_tagger to\n",
      "[nltk_data]     /home/nishant/nltk_data...\n",
      "[nltk_data]   Package averaged_perceptron_tagger is already up-to-\n",
      "[nltk_data]       date!\n"
     ]
    }
   ],
   "source": [
    "import nltk\n",
    "import pandas\n",
    "import datetime\n",
    "import math\n",
    "from datetime import datetime\n",
    "nltk.download('wordnet')\n",
    "nltk.download('averaged_perceptron_tagger')\n",
    "import time\n",
    "import string\n",
    "from nltk.tokenize import RegexpTokenizer\n",
    "from nltk import WordNetLemmatizer\n",
    "from nltk.lm import Vocabulary\n",
    "from nltk.corpus import wordnet\n",
    "wnLemmatizer=WordNetLemmatizer()\n",
    "from math import log10\n",
    "import math\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "\n",
    "model_file=\"model-2018.txt\"\n",
    "result_file=\"baseline-result.txt\"\n",
    "\n",
    "## Experiment 2 : Stop Words ##################################################################################\n",
    "stopwords_filter=False\n",
    "\n",
    "## Experiment 3 : Word Length Filtering #######################################################################\n",
    "wordlength_filter=False\n",
    "\n",
    "## Experiment 4 : Infrequent Word Filtering  ##################################################################\n",
    "infreqword_filter_a=False\n",
    "infreqword_filter_b=False\n",
    "\n",
    "## Experiment 5 : Smoothing  ##################################################################################\n",
    "delta=0.5\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## to change the pos tags to the wordnet tags\n",
    "\n",
    "def get_taging_value(tagging):\n",
    "    if tagging.upper().startswith(\"N\"):\n",
    "        return wordnet.NOUN\n",
    "    elif tagging.upper().startswith(\"V\"):\n",
    "        return wordnet.VERB\n",
    "    elif tagging.upper().startswith(\"J\"):\n",
    "        return wordnet.ADJ\n",
    "    elif tagging.upper().startswith(\"R\"):\n",
    "        return wordnet.ADV\n",
    "    else: \n",
    "        return wordnet.NOUN;"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[]"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Experiment 2: Stop word\n",
    "stopwords_file = open('stopWords.txt', 'r',encoding=\"utf8\")\n",
    "stopwords = stopwords_file.readlines()\n",
    "stopwords_file.close()\n",
    "for word_loop in range(0,len(stopwords)):\n",
    "        stopwords[word_loop]=stopwords[word_loop].strip()\n",
    "if stopwords_filter:\n",
    "    model_file=\"stopword-model.txt\"\n",
    "    result_file=\"stopword-result.txt\"\n",
    "else:\n",
    "    stopwords=[]\n",
    "  \n",
    "(stopwords)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Experiment 3 : Word Length Filtering \n",
    "if wordlength_filter:\n",
    "    model_file=\"wordlength-model.txt\"\n",
    "    result_file=\"wordlength-result.txt\"\n",
    "\n",
    "word_length_words=set()\n",
    "def word_length_filtering(word_inp):\n",
    "    bool_result=len(word_inp)>2  and len(word_inp)<9\n",
    "    if not bool_result:\n",
    "        word_length_words.add(word_inp)\n",
    "    if wordlength_filter:\n",
    "        return bool_result\n",
    "    else:\n",
    "        return True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "##check is start and end is characaters and no digits in between\n",
    "def check_word_correct(word_inp):\n",
    "    word_inp=word_inp.lower()\n",
    "    bool_word_filter=True\n",
    "    if wordlength_filter:\n",
    "        bool_word_filter=word_length_filtering(word_inp)\n",
    "    return (not any(ch.isdigit() for ch in word_inp)) and (not word_inp in stopwords) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "##initial values\n",
    "start_time=time.process_time()\n",
    "start_time=time.time()\n",
    "regExpTokenizer=RegexpTokenizer(r'\\w+')\n",
    "vocab_per_post_type=dict()\n",
    "vocab_per_post_type['story']=dict()\n",
    "vocab_per_post_type['ask_hn']=dict()\n",
    "vocab_per_post_type['show_hn']=dict()\n",
    "vocab_per_post_type['poll']=dict()\n",
    "post_type_len=dict()\n",
    "post_type_len['story']=0\n",
    "post_type_len['ask_hn']=0\n",
    "post_type_len['show_hn']=0\n",
    "post_type_len['poll']=0\n",
    "\n",
    "##vocabulary\n",
    "vocabulary=set()\n",
    "vocab_with_freq=dict()\n",
    "##remove words\n",
    "remove_words=set()\n",
    "\n",
    "##number of training records\n",
    "train_records=0\n",
    "\n",
    "##list of test data\n",
    "test_data=[]\n",
    "\n",
    "input_pandas = pandas.read_csv(\"hn2018_2019.csv\", encoding=\"utf8\")\n",
    "input_pandas['Created At'] = pandas.to_datetime(input_pandas['Created At'])\n",
    "input_pandas['Created At']= input_pandas['Created At'].dt.year"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'story': 254222, 'ask_hn': 12509, 'show_hn': 10225, 'poll': 25}\n",
      "276981\n",
      "361.01934146881104\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "368819"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from nltk import bigrams\n",
    "\n",
    "\n",
    "for index,pandas_row in input_pandas.iterrows():\n",
    "    loop_row=dict(pandas_row)\n",
    "    year=loop_row[\"Created At\"]\n",
    "    \n",
    "    \n",
    "    ## tokenization\n",
    "    token_words=regExpTokenizer.tokenize(loop_row[\"Title\"])\n",
    "    temp_words=nltk.pos_tag(token_words)\n",
    "    ## lemmatization\n",
    "    train_words=[wnLemmatizer.lemmatize(w[0], get_taging_value(w[1])) for w in temp_words]\n",
    "    \n",
    "    if year==2018:\n",
    "        line_vocab=list()\n",
    "        line_remove=set()\n",
    "        ##bigram\n",
    "        bigrams=list(nltk.bigrams(token_words))\n",
    "        bigram_counter=0\n",
    "        last_added=False\n",
    "        for bigram in bigrams:\n",
    "            word1,tag1=temp_words[bigram_counter]\n",
    "            word2,tag2=temp_words[bigram_counter+1]\n",
    "            lemma1=train_words[bigram_counter]\n",
    "            lemma2=train_words[bigram_counter+1]\n",
    "            if tag1.startswith(\"N\")  and  tag2.startswith(\"N\"):\n",
    "                if check_word_correct(lemma1) and check_word_correct(lemma2) and check_word_correct(word1) and check_word_correct(word2):\n",
    "                    bi=lemma1+\" \"+lemma2\n",
    "                    line_vocab.append(bi)\n",
    "                    last_added=True\n",
    "                elif check_word_correct(lemma1) and check_word_correct(word1):\n",
    "                    line_vocab.append(lemma1)\n",
    "                    last_added=False\n",
    "                else:\n",
    "                    line_remove.add(lemma1)\n",
    "                    last_added=False\n",
    "                \n",
    "                if bigram_counter==len(train_words)-2:\n",
    "                    if check_word_correct(lemma2) and check_word_correct(word2):\n",
    "                        line_vocab.append(lemma2)\n",
    "                    else:\n",
    "                        line_remove.add(lemma2)\n",
    "            else:\n",
    "                ##unigram\n",
    "                if not last_added:\n",
    "                    if check_word_correct(lemma1) and check_word_correct(word1):\n",
    "                        line_vocab.append(lemma1)\n",
    "                    else:\n",
    "                        line_remove.add(lemma1)\n",
    "                last_added=False\n",
    "                if bigram_counter==len(train_words)-2:\n",
    "                    if check_word_correct(lemma2) and check_word_correct(word2):\n",
    "                        line_vocab.append(lemma2)\n",
    "                    else:\n",
    "                        line_remove.add(lemma2)\n",
    "            bigram_counter=bigram_counter+1  \n",
    "        \n",
    "        for rw in line_remove:\n",
    "            rw=rw.lower()\n",
    "            remove_words.add(rw)\n",
    "            \n",
    "        for voc in line_vocab:\n",
    "            voc=voc.lower()\n",
    "            if word_length_filtering(voc):\n",
    "                vocabulary.add(voc)\n",
    "                 \n",
    "        \n",
    "            ####### for Experiment 4 ##############################################\n",
    "            if voc not in vocab_with_freq:\n",
    "                vocab_with_freq[voc]=1\n",
    "            else:\n",
    "                vocab_with_freq[voc]=vocab_with_freq[voc]+1\n",
    "            #######################################################################\n",
    "            if loop_row[\"Post Type\"] in vocab_per_post_type:\n",
    "                if voc in vocab_per_post_type[loop_row[\"Post Type\"]]:\n",
    "                    vocab_per_post_type[loop_row[\"Post Type\"]][voc]=vocab_per_post_type[loop_row[\"Post Type\"]][voc]+1\n",
    "                else:\n",
    "                    vocab_per_post_type[loop_row[\"Post Type\"]][voc]=1\n",
    "            else:\n",
    "                vocab_per_post_type[loop_row[\"Post Type\"]]=dict()\n",
    "                vocab_per_post_type[loop_row[\"Post Type\"]][voc]=1\n",
    "        \n",
    "        \n",
    "        if loop_row[\"Post Type\"] in post_type_len:\n",
    "            post_type_len[loop_row[\"Post Type\"]]=post_type_len[loop_row[\"Post Type\"]]+1\n",
    "        else:\n",
    "            post_type_len[loop_row[\"Post Type\"]]=1\n",
    "        train_records=train_records+1\n",
    "    elif year==2019:\n",
    "        test_data.append(loop_row)\n",
    "print(post_type_len)\n",
    "print(train_records)\n",
    "print(time.time()-start_time)\n",
    "\n",
    "if stopwords_filter:\n",
    "    for stpwrd in stopwords:\n",
    "        if stpwrd in remove_words:\n",
    "            remove_words.remove(stpwrd)\n",
    "if wordlength_filter:\n",
    "    for wrdftr in word_length_words:\n",
    "        #print(wrdftr)\n",
    "        if wrdftr in remove_words:\n",
    "            remove_words.remove(wrdftr)\n",
    "(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Experiment 4 Part a ##########################\n",
    "\n",
    "if infreqword_filter_a:\n",
    "    vocab_freq_sorted = sorted(vocab_with_freq.items(), key=lambda key_value: key_value[1])\n",
    "    emp4a_remove=set()\n",
    "    min_freq=20\n",
    "    for word, freq in vocab_freq_sorted:\n",
    "         if freq<=min_freq:\n",
    "            emp4a_remove.add(word)\n",
    "\n",
    "    for key in vocab_per_post_type.keys():\n",
    "        for word in emp4a_remove:\n",
    "            if word in vocab_per_post_type[key]:\n",
    "                vocab_per_post_type[key][word]=0\n",
    "            \n",
    "    for word in emp4a_remove:\n",
    "        vocabulary.remove(word)\n",
    "    print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "############ Experiment 4 Part b ##########################\n",
    "\n",
    "\n",
    "if infreqword_filter_b:\n",
    "    vocab_freq_reverse = sorted(vocab_with_freq.items(), key=lambda key_value: key_value[1], reverse=True)\n",
    "    percentage=5\n",
    "    counter_max=math.floor((percentage*len(vocabulary))/100)\n",
    "\n",
    "    emp4b_remove=set()\n",
    "    counter_emp4=1\n",
    "\n",
    "    for word, freq in vocab_freq_reverse:\n",
    "        if counter_emp4<=counter_max:\n",
    "            emp4b_remove.add(word)\n",
    "        counter_emp4=counter_emp4+1\n",
    "\n",
    "    for key in vocab_per_post_type.keys():\n",
    "        for word in emp4b_remove:\n",
    "            if word in vocab_per_post_type[key]:\n",
    "                vocab_per_post_type[key][word]=0\n",
    "            \n",
    "    for key in emp4b_remove:\n",
    "        vocabulary.remove(key)\n",
    "    print(len(vocabulary))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "story 1879011\n",
      "ask_hn 115744\n",
      "show_hn 86845\n",
      "poll 197\n",
      "368819\n"
     ]
    }
   ],
   "source": [
    "## storing the model data in model-2018.txt\n",
    "vocabulary_len=len(vocabulary)\n",
    "vocab_sizes=dict()\n",
    "vocab_sizes['story']=0\n",
    "vocab_sizes['ask_hn']=0\n",
    "vocab_sizes['show_hn']=0\n",
    "vocab_sizes['poll']=0\n",
    "\n",
    "for key in vocab_per_post_type.keys():\n",
    "    vocab_sizes[key]=0\n",
    "    for word_count in vocab_per_post_type[key]:\n",
    "        vocab_sizes[key]=vocab_sizes[key]+vocab_per_post_type[key][word_count]\n",
    "    print(key,vocab_sizes[key])\n",
    "    \n",
    "vocabulary=sorted(vocabulary)\n",
    "prob_word=[]\n",
    "for vocab_word in vocabulary:\n",
    "    word_loop=dict()\n",
    "    word_loop['word']=vocab_word\n",
    "    for key in vocab_per_post_type.keys():\n",
    "        if not vocab_word in vocab_per_post_type[key]:\n",
    "            vocab_per_post_type[key][vocab_word]=0\n",
    "        word_probability=(vocab_per_post_type[key][vocab_word]+delta)/(vocab_sizes[key]+(vocabulary_len))\n",
    "        ## frequency,probability\n",
    "        word_loop[key]=(vocab_per_post_type[key][vocab_word],log10(word_probability))\n",
    "    prob_word.append(word_loop)\n",
    "print(vocabulary_len)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "## saving vocabulary file\n",
    "vocabulary_file= open(\"vocabulary.txt\",\"w+\", encoding=\"utf-8\")\n",
    "for vocab_word in vocabulary:\n",
    "    vocabulary_file.write(\"%s\\n\"%(vocab_word))\n",
    "vocabulary_file.close()    \n",
    "\n",
    "## saving remove word file\n",
    "removeword_file= open(\"remove_word.txt\",\"w+\", encoding=\"utf-8\")\n",
    "for removeword in remove_words:\n",
    "    removeword_file.write(\"%s\\n\"%(removeword))\n",
    "removeword_file.close() \n",
    "\n",
    "## saving model-2018\n",
    "model_train_file= open(model_file,\"w+\", encoding=\"utf-8\")\n",
    "counter=1\n",
    "for loop in prob_word:\n",
    "    model_train_file.write(\"%d  %s  %d  %f  %d  %f  %d  %f  %d  %f\\n\" %(counter, loop['word'], loop['story'][0], loop['story'][1], loop['ask_hn'][0], loop['ask_hn'][1], loop['show_hn'][0], loop['show_hn'][1], loop['poll'][0], loop['poll'][1]))\n",
    "    counter=counter+1\n",
    "model_train_file.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "vocab_dict=dict.fromkeys(vocabulary,0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "      ask_hn       0.81      1.00      0.89      5454\n",
      "        poll       1.00      0.17      0.29         6\n",
      "     show_hn       0.92      1.00      0.96      4903\n",
      "       story       1.00      0.99      0.99    126852\n",
      "\n",
      "    accuracy                           0.99    137215\n",
      "   macro avg       0.93      0.79      0.78    137215\n",
      "weighted avg       0.99      0.99      0.99    137215\n",
      "\n",
      "Confusion Matrix\n",
      "[[  5453      0      0      1]\n",
      " [     0      1      1      4]\n",
      " [     0      0   4903      0]\n",
      " [  1319      0    435 125098]]\n",
      "Per class wise accuracy:\n",
      "[0.99981665 0.16666667 1.         0.98617286]\n",
      "Overall Accuracy: 98.71734139853514\n"
     ]
    }
   ],
   "source": [
    "\n",
    "test_count=0\n",
    "test_right=0\n",
    "start_time=time.time()\n",
    "output_given=list()\n",
    "output_pred=list()\n",
    "output_lines=list()\n",
    "counter=1\n",
    "for loop_row in test_data:\n",
    "    output_one_line=dict()\n",
    "    output_one_line['counter']=counter\n",
    "    output_one_line['title']=loop_row[\"Title\"]\n",
    "    output_one_line['trueval']=loop_row[\"Post Type\"]\n",
    "    token_words=regExpTokenizer.tokenize(loop_row[\"Title\"])\n",
    "    output_given.append(loop_row[\"Post Type\"])\n",
    "    temp_words=nltk.pos_tag(token_words)\n",
    "    train_words=[wnLemmatizer.lemmatize(w[0], get_taging_value(w[1])) for w in temp_words]\n",
    "    line_vocab=list()    \n",
    "    ##bigram\n",
    "    bigrams=list(nltk.bigrams(token_words))\n",
    "    bigram_counter=0\n",
    "    last_added=False\n",
    "    for bigram in bigrams:\n",
    "        word1,tag1=temp_words[bigram_counter]\n",
    "        word2,tag2=temp_words[bigram_counter+1]\n",
    "        lemma1=train_words[bigram_counter]\n",
    "        lemma2=train_words[bigram_counter+1]\n",
    "        if tag1.startswith(\"N\")  and  tag2.startswith(\"N\"):\n",
    "            if check_word_correct(lemma1) and check_word_correct(lemma2) and check_word_correct(word1) and check_word_correct(word2):\n",
    "                bi=lemma1+\" \"+lemma2\n",
    "                line_vocab.append(bi)\n",
    "                last_added=True\n",
    "            elif check_word_correct(lemma1) and check_word_correct(word1):\n",
    "                line_vocab.append(lemma1)\n",
    "                last_added=False\n",
    "            else:\n",
    "                last_added=False\n",
    "                \n",
    "            if bigram_counter==len(train_words)-2:\n",
    "                if check_word_correct(lemma2) and check_word_correct(word2):\n",
    "                    line_vocab.append(lemma2)\n",
    "        else:\n",
    "            ##unigram\n",
    "            if not last_added:\n",
    "                if check_word_correct(lemma1) and check_word_correct(word1):\n",
    "                    line_vocab.append(lemma1)\n",
    "            last_added=False\n",
    "            if bigram_counter==len(train_words)-2:\n",
    "                if check_word_correct(lemma2) and check_word_correct(word2):\n",
    "                    line_vocab.append(lemma2)\n",
    "    \n",
    "    result_dict=dict()\n",
    "    for voc in line_vocab:\n",
    "        if word_length_filtering(voc):\n",
    "            line_vocab.remove(voc)\n",
    "    for key in vocab_per_post_type.keys():\n",
    "        result_dict[key]=log10(post_type_len[key]/train_records)\n",
    "        \n",
    "    for key in vocab_per_post_type.keys():\n",
    "        for voc in line_vocab:\n",
    "            voc=voc.lower()\n",
    "            if voc in vocab_per_post_type[key]:\n",
    "                prob=((vocab_per_post_type[key][voc]+delta)/(vocab_sizes[key]+(vocabulary_len)))\n",
    "                result_dict[key]=result_dict[key]+log10(prob)\n",
    "            #else:\n",
    "             #   voc_arr=voc.split(\" \")\n",
    "              #  for loop_arr in voc_arr:\n",
    "               #     if check_word_correct(loop_arr):\n",
    "                #        prob=(0.5/(vocab_sizes[key]+(delta*vocabulary_len)))\n",
    "                 #       result_dict[key]=result_dict[key]+log10(prob)\n",
    "    \n",
    "    ans=max(result_dict, key=result_dict.get)\n",
    "    output_one_line['pred']=ans\n",
    "    output_one_line['story']=result_dict['story']\n",
    "    output_one_line['show_hn']=result_dict['show_hn']\n",
    "    output_one_line['ask_hn']=result_dict['ask_hn']\n",
    "    output_one_line['poll']=result_dict['poll']\n",
    "    output_one_line['ans']=\"right\" if (loop_row[\"Post Type\"]==ans) else \"wrong\"\n",
    "    output_pred.append(ans) \n",
    "    output_lines.append(output_one_line)\n",
    "    counter=counter+1\n",
    "##  \n",
    "import numpy as np\n",
    "from sklearn.metrics import classification_report\n",
    "print(classification_report(output_given, output_pred))\n",
    "from sklearn.metrics import confusion_matrix\n",
    "varFroCM=confusion_matrix(output_given, output_pred)\n",
    "print(\"Confusion Matrix\")\n",
    "print(varFroCM)\n",
    "varFroCM= varFroCM.astype('float') / varFroCM.sum(axis=1)[:, np.newaxis]\n",
    "print(\"Per class wise accuracy:\")\n",
    "print(varFroCM.diagonal())\n",
    "print(\"Overall Accuracy:\",accuracy_score(output_given,output_pred)*100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "output_file= open(result_file,\"w+\", encoding=\"utf-8\")\n",
    "for output_one_line in output_lines:\n",
    "    output_file.write(\"%d  %s  %s  %.10f  %.10f  %.10f  %.10f  %s  %s\\n\\r\" \n",
    "                      %(output_one_line['counter'], output_one_line['title'], \n",
    "                        output_one_line['pred'],output_one_line['story'],output_one_line['ask_hn'],\n",
    "                        output_one_line['show_hn'],output_one_line['poll'],\n",
    "                       output_one_line['trueval'],output_one_line['ans']))\n",
    "    counter=counter+1\n",
    "output_file.close()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
